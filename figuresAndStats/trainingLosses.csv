Epoch,Training Loss,Validation Loss
0,0.20682836,0.2535058
1,0.16754931,0.21374084
2,0.13964838,0.20098987
3,0.11664377,0.15298428
4,0.20102014,0.31060788
5,0.098763,0.1360022
6,0.10561408,0.14503281
7,0.11365944,0.16118242
8,0.12220438,0.15564005
9,0.10016105,0.14028724
10,0.08764699,0.1206774
11,0.11528291,0.12092954
12,0.07703162,0.11188453
13,0.09250823,0.11411412
14,0.090219535,0.15346396
15,0.06529543,0.108846284
16,0.06941421,0.12330525
17,0.07107033,0.10579095
18,0.06852811,0.099469006
19,0.063129425,0.09670832
20,0.063559525,0.101052046
21,0.06315318,0.092658766
22,0.060514905,0.10534948
23,0.05655038,0.09667671
24,0.06420189,0.10715682
25,0.062111106,0.09857565
26,0.060485646,0.08784843
27,0.06264383,0.11515893
28,0.05591013,0.106950894
29,0.053987242,0.09830608
30,0.053384762,0.10110306
31,0.056573227,0.11458389
32,0.059365846,0.123955965
33,0.0503335,0.094570026
34,0.048252694,0.09530037
35,0.047191948,0.092540406
36,0.060485646,0.08784843
